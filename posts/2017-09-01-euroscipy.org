#+title: EuroScipy 2017
#+date: <2017-09-01 00:00>

The second conference I attended this year was the [[https://www.euroscipy.org/2017/][EuroScipy 2017]] in Erlangen. I gave a [[https://www.youtube.com/watch?v=mc8ru37dwf8][Talk about Audio in Python]] and a [[https://youtu.be/qTgk2DUM6G0?t=10m15s][Lightning Talk about Transplant]].

My most striking impression of EuroScipy is that every person I talked to was working on something interesting, and could talk about his/her topic clearly and with enthusiasm. This mirrors my feelings from last year's Chaos Communication Congress, where the short scientific section stood out for its clarity and passion. I also enjoyed the fact that attendees were international and diverse, and exuded a heart-warming sense of community.

But more than that, everyone seemed to use a common set of methods (statistical methods, signal processing, machine learning), even in wildly differing areas of research. And this translated to technology stacks as well. Even though each scientific discipline of course uses their own data sets, features, and models, absolutely everybody used [[http://jupyter.org/][Jupyter Notebooks]] for tutorials and teaching, and most of the data analyses were done in [[http://pandas.pydata.org/][Pandas]]. This is particularly heartening since these technologies are clearly geared towards reproducible research and open data.

The hot topic of the conference clearly was machine learning and neural networks. Hopefully, this whole ecosystem of competing frameworks will eventually reach its NumPy moment, and collapse into a single, unified package. It is my hope that neural networks will eventually find their place as just another machine learning method with a few reusable parametrized prototype implementations in scikit-learn. The current confusion of competing frameworks and network architectures does not seem to be a good long-term solution. Tools like [[https://keras.io/][Keras]] look like good steps towards this goal.

Finally, there was a lot of talk about “the reproducability crisis” in science, and possible steps to improve the scientific process. In particular, I learned that it is absolutely necessary to never look at your test data before the final evaluation, to avoid overfitting your brain. You need to split your data into a development set for training, an evaluation set for parameter tuning, and a totally separate testing set for the final evaluation. In a similar way, it is important to state your hypotheses /in writing/ before you test them, to avoid “HARKing” (Hypothesis After the Results are Known; aka “Noise Mining”, “P-Hacking”, or “Procedural Overfitting”). I dearly hope that [[https://cos.io/rr/][Registered Reports]] will catch on, and absolve us from these all too human biases.

In conclusion, EuroScipy 2017 was a ton of fun, and educational in many ways that I did not expect.
